# DenseKD: Dense Knowledge Distillation by Exploiting Region and Sample Importance



## Introduction

跨层蒸馏CKD先前方法通过选择教师的粗粒度阶段特征来教授学生，导致通道对齐不当，这些方法中的大多数对所有知识进行统一蒸馏，限制了学生更多关注重要知识。

我们构建了可学习的密集框架，是每个学生的每个通道都能灵活的从教师那里捕获更多元的通道特征

![image-20250516161036129](imgs/image-20250516161036129.png)

问题：

- 粗粒度蒸馏位置选择：先前跨层蒸馏方法进行阶段性选择，以匹配具有单个教师特征的多个学生特征，然而，如上图a所示，DNN中每个阶段的通道特征是多种多样的，倾向于灵活的捕捉教师模型中不同阶段的通道特征。分阶段选择对同一阶段中的所有特征执行相同的操作，然后任何通道的知识传递都会收到同阶段其他通道的限制，导致特征对齐不准确；
- 所有区域均匀蒸馏：一些方法忽略了不同局部区域对知识转移的影响，如上图b所示，同一样本中的不同区域在对象识别中骑着不同的作用，因此他们往往具有不同的潜力来指导学生模型的学习，并且所有区域的统一蒸馏使学生模型无法捕捉到不同区域的差异；
- 所有样本均匀蒸馏：对于分类任务中的KD，大量方法认为所有样本对特招生转移的影响相同，然而如上图c所示，DNN表现出对不同类别的样本进行分类的不同能力，这表明网络对不同样本具有不同的特征提取能力，因此数据集中的不同样本具有不同的蒸馏指导潜力，以统一的方式传输所有样本会限制学生网络从重要样本中获取信息

为解决上述问题，本文提出一种细粒度密集跨层知识蒸馏DenseKD，并研究了特征对齐过程中的区域和样本的重要性：

- 提出一种可学习的密集连接，以在自适应和细粒度的方案中准确执行跨层蒸馏；
- 通过教师特征的变化计算区域重要性用于研究不同区域对蒸馏的指导潜力；
- 利用教师损失计算样本的重要性来说明不同样本的蒸馏指导潜力

## Method

![image-20250516163536663](imgs/image-20250516163536663.png)

#### Distillation with Dense Connection

CKD方法选择教师的多阶段特征来监督学生模型的每个阶段的学习，CKD损失可以表示为：
$$
L_{CKD_i} = \sum^{L_s}_{l_s=1}\sum^{L_t}_{l_t=1}D(T^t_{l_t}(F^t_{l_t}(x_i)), T^s_{l_s}(F^s_{l_s}(x_i)))
$$
交换师生模型求和顺序，重写上式：
$$
L_{CKD_i} = \sum^{L_t}_{l_t=1}\sum^{L_s}_{l_s=1}D(T^t_{l_t}(F^t_{l_t}(x_i)), T^s_{l_s}(F^s_{l_s}(x_i)))
$$
上式表明每个教师阶段的特征可以指导学生的多阶段学习，学生同一阶段的通道性特征可以被教师不同阶段的通道性特征所监督，因此先前的阶段性选择忽略了同一阶段特征的多样性，导致师生对之间的蒸馏路径不一致。因此我们构建了可学习的密集架构，以细粒度的方式将学生的多个阶段与每个阶段的教师相匹配。

构建的连接需要对不同学生阶段每个频道的特征进行选择、融合和变换，以匹配第l层教师特征