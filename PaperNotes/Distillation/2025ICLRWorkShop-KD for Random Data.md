# Knowledge Distillation for Random Data: Soft Labels and Similarity Scores May Contain Memorized Information

2025ICLRWorkShop



## Introduction

一个常见的假设是logits是有益的，他们已经编码了一个可以很好表示数据的隐藏结构，很多文献对神经网路如何存储关联进行研究，但关于如何传输和压缩记忆的工作相对较少，因此我们提出：教师的logits是否对记忆的知识进行编码？如果是，学生可以获取这些重要信息么？

有相关工作证明，在随机数据上训练的网络仍会拾取泛化的图像特征，由于我们想要传输记忆数据，因此我们考虑记住有限数量的随机输入标签对的教师网络，并检查从中提炼出的知识。我们通过logit或相似性分数来训练学生，学生只能访问记忆数据的子集，并在另一个保留的部分进行测试

- 通过对教师logits的知识蒸馏和训练，学生确实可以获得有关记忆随机数据的重要信息，而无需完全访问完整的记忆数据集；
- 为了让学生成功，有限随机数据集需要在输入维度、每个类别的标签数量和维度的总数之间确定平衡，能力较强的学生会获得更高的测试准确性；
- 除了对logits进行训练之外，训练学生匹配来自教师模型的logits嵌入空间中的成对余弦相似度可以导致类似记忆数据的传输，但此方法效率较低，且导致正确的预测较少

因此我们认为教师logits可以包含和传递我们直觉上认为被记住的信息，因为原始数据是随机抽样的。
