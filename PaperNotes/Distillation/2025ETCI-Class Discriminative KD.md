# Class Discriminative Knowledge Distillation

2025ETCI	no code 	-	20250508





## Introduction

我们认为仅将类别信息表示为类向量可能无法完全捕获其固有属性，基于logit的蒸馏应该提供与基于特征的方法相当的性能，因为logit在更高的语义级别上运行。我们认为logit矩阵的列向量应该在KD过程中表现出可分离性以传达于语义信息，且一个蒸馏良好的学生模型应该保留一个嵌入空间来捕捉教师的语义结构，所有类别表示都应该靠近其教师对应项并原理教师的不同类别嵌入。

DIST忽视了一个问题，即logit矩阵的列向量不能有效的反应类别语义，这可能会损害蒸馏，我们加入了两个额外的正则化：可分离性和正交性，以增强类感知的logit蒸馏。

我们将我们提出的方法命名为类判别性知识蒸馏CD-KD

- 在类感知知识蒸馏中提出两个正则化属于，利用可分离性和正交化技术，能够有效的从教师模型中传递语义类级知识
- 提出了不对称架构设计，包含一个额外的投影层，并为教师重新利用了学生分类器，logit空间中这种对齐有助于在类感知知识传递过程中增强语义理解

## Method

#### Class-Level Distillation

类表示中编码的信息，也具有相当大的意义。DIST是主要的baseline，引入了利用logit矩阵的列向量来传递类级知识概念，在实例级别和类别级别传递知识的蒸馏损失表示为：
$$
L_{KD}=L_{ins}+\lambda L_{cls} \\
L_{ins} = D(Z_S, Z_T) \\
L_{cls} = D(Z^T_S, Z^T_T)
$$
这个蒸馏过程包括两个基本组成部分，实例方面表示为$L_{ins}$. 我们从这个框架出发，丢弃传统的kl散度，并使用归一化均方误差N-MSE，来衡量师生模型之间预测对数的差异，考虑到师生模型大小和参数量方面的巨大差异，他们的输出范数不太可能具有可比的两级，因此我们计算了归一化后的师生模型分布的MSE损失，N-MSE不仅在量化师生logit差异优于MSE，并且在捕获师生模型之间的余弦相似性也优于MSE：
$$
D(p,z) = ||\frac{p}{||p||_2} - \frac{z}{||z||_2}||^2_2 = 2 - 2·\frac{<p,z>}{||p||_2, ||z||_2}
$$

#### Regularization Terms 

